# -*- coding: utf-8 -*-
"""Vector-Based-RAG-Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17O0Rmeu_89naZZVolD29J7bz4pyf2T-H

**Install Required Dependencies**
"""

!pip install \
  openai==1.30.5 \
  langchain==0.2.2 \
  langchain-community==0.2.3 \
  langchain-openai==0.1.8 \
  chromadb==0.5.0 \
  tiktoken \
  python-dotenv \
  "unstructured[md]"

"""**Mount Google Drive**"""

from google.colab import drive
drive.mount("/content/drive")

"""**Import Libraries and Configure Data & Vector Store Paths**"""

from langchain_community.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
import os, shutil

DATA_PATH = "/content/drive/MyDrive/Data"
CHROMA_PATH = "/content/chroma_db"

print("Files:", os.listdir(DATA_PATH))

# Reset DB
if os.path.exists(CHROMA_PATH):
    shutil.rmtree(CHROMA_PATH)

loader = DirectoryLoader(DATA_PATH, glob="*.md")
documents = loader.load()

splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=100
)
chunks = splitter.split_documents(documents)

embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

db = Chroma.from_documents(
    documents=chunks,
    embedding=embedding,
    persist_directory=CHROMA_PATH
)

db.persist()
print("âœ… Chroma DB created (FREE embeddings)")

from langchain_community.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

model_name = "google/flan-t5-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300
)

llm = HuggingFacePipeline(pipeline=pipe)

"""**QUERY THE RAG CHATBOT**"""

from langchain_community.vectorstores import Chroma

def ask_rag(question):
    db = Chroma(
        persist_directory=CHROMA_PATH,
        embedding_function=embedding
    )

    docs = db.similarity_search(question, k=3)

    context = "\n\n---\n\n".join(doc.page_content for doc in docs)

    prompt = f"""
Answer ONLY using the context below.

Context:
{context}

Question:
{question}
"""

    return llm.invoke(prompt)

"""**Ask a Question**"""

ask_rag("How does Alice meet the White Rabbit?")